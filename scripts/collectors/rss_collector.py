#!/usr/bin/env python3
"""
RSS Feed Collector
ë‹¤ì–‘í•œ Tech ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ RSS í”¼ë“œ ìˆ˜ì§‘
- fastfeedparser ì‚¬ìš© (feedparserë³´ë‹¤ 10ë°° ë¹ ë¦„)
"""

import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv(Path(__file__).parent.parent / '.env')

try:
    import fastfeedparser
    PARSER_AVAILABLE = True
except ImportError:
    PARSER_AVAILABLE = False
    print("âš ï¸ fastfeedparser ë¯¸ì„¤ì¹˜: pip install fastfeedparser")

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False


# í”¼ë“œ ì†ŒìŠ¤ ì •ì˜
RSS_FEEDS = {
    # í•œêµ­ì–´ (ìš°ì„ )
    "GeekNews": {
        "url": "https://news.hada.io/rss",
        "lang": "ko",
        "category": "general"
    },

    # ì˜ë¬¸ ì£¼ìš” ë‰´ìŠ¤
    "TechCrunch": {
        "url": "https://techcrunch.com/feed/",
        "lang": "en",
        "category": "general"
    },
    "The Verge": {
        "url": "https://www.theverge.com/rss/index.xml",
        "lang": "en",
        "category": "general"
    },
    "Ars Technica": {
        "url": "https://feeds.arstechnica.com/arstechnica/index",
        "lang": "en",
        "category": "general"
    },
    "Wired": {
        "url": "https://www.wired.com/feed/rss",
        "lang": "en",
        "category": "general"
    },

    # ê°œë°œì íŠ¹í™”
    "Hacker News": {
        "url": "https://hnrss.org/frontpage",  # ë¹„ê³µì‹ RSS
        "lang": "en",
        "category": "dev"
    },

    # ì¶”ê°€ ì†ŒìŠ¤
    "MIT Tech Review": {
        "url": "https://www.technologyreview.com/feed/",
        "lang": "en",
        "category": "research"
    },
    "CNET": {
        "url": "https://www.cnet.com/rss/news/",
        "lang": "en",
        "category": "general"
    },
    "Engadget": {
        "url": "https://www.engadget.com/rss.xml",
        "lang": "en",
        "category": "general"
    },
}


class RSSCollector:
    def __init__(self, use_ai_summary: bool = True, max_per_source: int = 5):
        """
        Args:
            use_ai_summary: Geminië¡œ í•œê¸€ ìš”ì•½ ìƒì„±
            max_per_source: ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
        """
        if not PARSER_AVAILABLE:
            raise ImportError("fastfeedparserê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install fastfeedparser")

        self.use_ai_summary = use_ai_summary
        self.max_per_source = max_per_source
        self.model = None

        # Gemini ì„¤ì •
        if use_ai_summary and GEMINI_AVAILABLE:
            api_key = os.getenv('GEMINI_API_KEY')
            if api_key:
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel('gemini-2.0-flash')
                print("âœ… Gemini ìš”ì•½ í™œì„±í™”")

    def summarize_korean(self, title: str, description: str = "") -> str:
        """Geminië¡œ í•œê¸€ í•œì¤„ ìš”ì•½"""
        if not self.model:
            return ""

        try:
            text = f"{title}. {description[:300]}" if description else title
            prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©/ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•œ ì¤„(25ì ì´ë‚´)ë¡œ ìš”ì•½í•´ì¤˜.
ì´ëª¨ì§€ ì—†ì´, í•µì‹¬ë§Œ ê°„ë‹¨íˆ.

ë‚´ìš©: {text}

í•œì¤„ìš”ì•½:"""

            response = self.model.generate_content(prompt)
            result = response.text.strip()
            return result.split('\n')[0]
        except Exception as e:
            print(f"  âš ï¸ ìš”ì•½ ì‹¤íŒ¨: {e}")
            return ""

    def fetch_feed(self, name: str, feed_info: dict) -> list:
        """ë‹¨ì¼ í”¼ë“œ ìˆ˜ì§‘"""
        url = feed_info["url"]
        print(f"  ğŸ“¡ {name} ìˆ˜ì§‘ ì¤‘...")

        try:
            feed = fastfeedparser.parse(url)

            if not feed or not feed.get('entries'):
                print(f"  âš ï¸ {name}: í•­ëª© ì—†ìŒ")
                return []

            articles = []
            entries = feed['entries'][:self.max_per_source]

            for entry in entries:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.get('title', 'No Title')
                link = entry.get('link', '')

                # ì„¤ëª… (HTML íƒœê·¸ ì œê±°ëŠ” ì„ íƒ)
                description = entry.get('summary', entry.get('description', ''))
                if description:
                    # ê°„ë‹¨í•œ HTML íƒœê·¸ ì œê±°
                    import re
                    description = re.sub(r'<[^>]+>', '', description)[:500]

                # ë°œí–‰ì¼
                published = entry.get('published', entry.get('updated', ''))

                articles.append({
                    'source': name,
                    'lang': feed_info['lang'],
                    'category': feed_info['category'],
                    'title': title,
                    'link': link,
                    'description': description,
                    'published': published,
                })

            print(f"  âœ… {name}: {len(articles)}ê°œ ìˆ˜ì§‘")
            return articles

        except Exception as e:
            print(f"  âŒ {name} ì˜¤ë¥˜: {e}")
            return []

    def collect_all(self, sources: list = None, categories: list = None) -> dict:
        """
        ëª¨ë“  RSS í”¼ë“œ ìˆ˜ì§‘

        Args:
            sources: íŠ¹ì • ì†ŒìŠ¤ë§Œ ìˆ˜ì§‘ (ì˜ˆ: ["GeekNews", "TechCrunch"])
            categories: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ (ì˜ˆ: ["dev", "general"])

        Returns:
            dict: {source_name: [articles]}
        """
        print(f"ğŸš€ RSS í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M')}")

        results = {}
        total_count = 0

        for name, feed_info in RSS_FEEDS.items():
            # í•„í„°ë§
            if sources and name not in sources:
                continue
            if categories and feed_info['category'] not in categories:
                continue

            articles = self.fetch_feed(name, feed_info)
            if articles:
                results[name] = articles
                total_count += len(articles)

        print(f"\nğŸ“Š ì´ {len(results)}ê°œ ì†ŒìŠ¤ì—ì„œ {total_count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

        # í•œê¸€ ìš”ì•½ ì¶”ê°€ (ì˜ë¬¸ ê¸°ì‚¬ë§Œ)
        if self.use_ai_summary and self.model:
            print("\nğŸ¤– ì˜ë¬¸ ê¸°ì‚¬ í•œê¸€ ìš”ì•½ ìƒì„± ì¤‘...")
            summary_count = 0

            for source, articles in results.items():
                for article in articles:
                    # ì˜ë¬¸ë§Œ ìš”ì•½ (í•œê¸€ì€ ì´ë¯¸ ì½ê¸° ì‰¬ì›€)
                    if article['lang'] == 'en' and summary_count < 20:  # API ì ˆì•½
                        summary = self.summarize_korean(article['title'], article['description'])
                        article['summary_kr'] = summary
                        if summary:
                            summary_count += 1
                            print(f"  âœ“ {article['title'][:30]}... â†’ {summary}")

        return results

    def format_markdown(self, results: dict, title: str = "Tech News Digest") -> str:
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""

        output = f"# {title}\n\n"
        output += f"> ìˆ˜ì§‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        output += "---\n\n"

        for source, articles in results.items():
            output += f"## ğŸ“° {source}\n\n"

            for i, article in enumerate(articles, 1):
                output += f"### {i}. [{article['title']}]({article['link']})\n\n"

                # í•œê¸€ ìš”ì•½ ë¨¼ì €
                if article.get('summary_kr'):
                    output += f"> ğŸ“Œ **{article['summary_kr']}**\n\n"

                if article['description']:
                    output += f"{article['description'][:200]}...\n\n"

                if article['published']:
                    output += f"ğŸ• {article['published']}\n\n"

            output += "---\n\n"

        return output

    def format_telegram(self, results: dict, max_items: int = 10) -> str:
        """í…”ë ˆê·¸ë¨ìš© í¬ë§· (ê°„ê²°í•˜ê²Œ)"""

        message = "ğŸ“° *Tech News Digest*\n\n"
        count = 0

        # í•œêµ­ì–´ ì†ŒìŠ¤ ë¨¼ì €
        for source in ["GeekNews"]:
            if source in results:
                message += f"*ğŸ‡°ğŸ‡· {source}*\n"
                for article in results[source][:3]:
                    message += f"â€¢ [{article['title'][:40]}...]({article['link']})\n"
                    count += 1
                message += "\n"

        # ì˜ë¬¸ ì†ŒìŠ¤
        for source, articles in results.items():
            if source == "GeekNews" or count >= max_items:
                continue

            message += f"*ğŸŒ {source}*\n"
            for article in articles[:2]:
                title = article['title'][:35]
                if article.get('summary_kr'):
                    message += f"â€¢ [{title}...]({article['link']})\n"
                    message += f"  â”” {article['summary_kr']}\n"
                else:
                    message += f"â€¢ [{title}...]({article['link']})\n"
                count += 1
                if count >= max_items:
                    break
            message += "\n"

        return message


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    collector = RSSCollector(use_ai_summary=True, max_per_source=3)

    # ëª¨ë“  í”¼ë“œ ìˆ˜ì§‘
    results = collector.collect_all()

    if results:
        # ë§ˆí¬ë‹¤ìš´ ì¶œë ¥
        md = collector.format_markdown(results)
        print("\n" + "=" * 60)
        print(md[:3000])

        # íŒŒì¼ ì €ì¥
        output_path = Path(__file__).parent.parent / 'output' / 'rss_news.md'
        output_path.parent.mkdir(exist_ok=True)
        output_path.write_text(md, encoding='utf-8')
        print(f"\nâœ… ì €ì¥ë¨: {output_path}")

        # í…”ë ˆê·¸ë¨ í¬ë§· ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“± í…”ë ˆê·¸ë¨ ë¯¸ë¦¬ë³´ê¸°:")
        print(collector.format_telegram(results))


if __name__ == "__main__":
    main()
